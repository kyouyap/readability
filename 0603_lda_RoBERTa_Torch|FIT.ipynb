{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "0603  lda RoBERTa Torch|FIT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1azzMaghNp3stfAPTK1PeCWSB16G3GsJK",
      "authorship_tag": "ABX9TyNtOcLYqU/TBvBk5WSq9U54",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyouyap/readability/blob/master/0603_lda_RoBERTa_Torch%7CFIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJoay0U4-s7r",
        "outputId": "5152d684-a382-4419-bfc9-90347605df66"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "if 'KAGGLE_URL_BASE' in set(os.environ.keys()):\n",
        "    train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
        "    test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
        "    print(\"train shape\",train.shape)\n",
        "# colaboratory環境ならTrue\n",
        "elif 'COLAB_GPU' in set(os.environ.keys()):\n",
        "    train = pd.read_csv(\"/content/drive/MyDrive/kaggle/readability/train.csv\")\n",
        "    test = pd.read_csv(\"/content/drive/MyDrive/kaggle/readability/test.csv\")\n",
        "    print(\"train shape\",train.shape)\n",
        "df=pd.concat([train,test])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape (2834, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGjk0-6_Bxom"
      },
      "source": [
        "from sklearn import model_selection\n",
        "def create_folds(data, num_splits):\n",
        "    data[\"kfold\"] = -1\n",
        "    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data)):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    return data\n",
        "train = create_folds(train, num_splits=5)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wilSYz37B1LR"
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from glob import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "gc.enable()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPnDb-W1B2fu"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import (\n",
        "    Dataset, DataLoader, \n",
        "    SequentialSampler, RandomSampler\n",
        ")\n",
        "!pip install -q transformers\n",
        "from transformers import AutoConfig\n",
        "from transformers import (\n",
        "    get_cosine_schedule_with_warmup, \n",
        "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
        ")\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "from transformers import MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm, trange\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D85S56E5aN5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997828a7-272c-4f3f-e8ec-cd84b2bfe3ef"
      },
      "source": [
        "!pip install lda\n",
        "import lda\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import joblib\n",
        "import time"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lda in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pbr<4,>=0.6 in /usr/local/lib/python3.7/dist-packages (from lda) (3.1.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from lda) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9GXSr0Rbqi2"
      },
      "source": [
        "\n",
        "bow_model = CountVectorizer(stop_words='english')\n",
        "bow = bow_model.fit_transform(df.excerpt)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-OWE_s4bzzD",
        "outputId": "1a1a0820-29f7-42d1-f25d-f8dcb89088ee"
      },
      "source": [
        "n = 20\n",
        "n_iter = 2000\n",
        "start = time.time()\n",
        "lda_model = lda.lda.LDA(n_topics=n, n_iter=n_iter, random_state=0, refresh=100)\n",
        "lda_model.fit(bow)\n",
        "# 初回実行時のみ保存\n",
        "joblib.dump(lda_model, 'lda_model_{}_{}iter.pkl'.format(n, n_iter))\n",
        "end = time.time()\n",
        "print(\"topic_N =\", str(n), \"train time\", end - start)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:lda:n_documents: 2841\n",
            "INFO:lda:vocab_size: 26560\n",
            "INFO:lda:n_words: 222894\n",
            "INFO:lda:n_topics: 20\n",
            "INFO:lda:n_iter: 2000\n",
            "INFO:lda:<0> log likelihood: -3048240\n",
            "INFO:lda:<100> log likelihood: -2071927\n",
            "INFO:lda:<200> log likelihood: -2056920\n",
            "INFO:lda:<300> log likelihood: -2050077\n",
            "INFO:lda:<400> log likelihood: -2047607\n",
            "INFO:lda:<500> log likelihood: -2045323\n",
            "INFO:lda:<600> log likelihood: -2043343\n",
            "INFO:lda:<700> log likelihood: -2043339\n",
            "INFO:lda:<800> log likelihood: -2041947\n",
            "INFO:lda:<900> log likelihood: -2041675\n",
            "INFO:lda:<1000> log likelihood: -2042541\n",
            "INFO:lda:<1100> log likelihood: -2042228\n",
            "INFO:lda:<1200> log likelihood: -2041705\n",
            "INFO:lda:<1300> log likelihood: -2040984\n",
            "INFO:lda:<1400> log likelihood: -2041800\n",
            "INFO:lda:<1500> log likelihood: -2041274\n",
            "INFO:lda:<1600> log likelihood: -2041307\n",
            "INFO:lda:<1700> log likelihood: -2040785\n",
            "INFO:lda:<1800> log likelihood: -2041372\n",
            "INFO:lda:<1900> log likelihood: -2041318\n",
            "INFO:lda:<1999> log likelihood: -2040047\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "topic_N = 20 train time 92.84610962867737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yslLt6hycg5u",
        "outputId": "d7199a4b-13e9-457f-d8c1-3b6fc1233405"
      },
      "source": [
        "joblib.dump(lda_model, 'lda_model_{}_{}iter.pkl'.format(n, n_iter))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lda_model_20_2000iter.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbYmuFWdcopC"
      },
      "source": [
        "lda_model = joblib.load('lda_model_20_2000iter.pkl')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBL3DoTccwbj"
      },
      "source": [
        "\n",
        "bow = bow_model.transform(df.excerpt)\n",
        "theta_docs_20 = lda_model.transform(bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOjugaogwMqz"
      },
      "source": [
        "\n",
        "train_theta = theta_docs_20[:len(train)]\n",
        "test_theta = theta_docs_20[len(train):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoYcvsFQwRnz"
      },
      "source": [
        "\n",
        "train_topic = train_theta.argmax(axis=1)\n",
        "test_topic = test_theta.argmax(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1yGEAQ-wTuQ"
      },
      "source": [
        "train['topic_id'] = train_topic\n",
        "test['topic_id'] = test_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zCA3FDzweKb"
      },
      "source": [
        "train_theta_df = pd.DataFrame(train_theta)\n",
        "train_theta_df.columns = [f'topic{i}' for i in range(train_theta.shape[1])]\n",
        "test_theta_df = pd.DataFrame(test_theta)\n",
        "test_theta_df.columns = [f'topic{i}' for i in range(test_theta.shape[1])]\n",
        "\n",
        "train = pd.concat([train, train_theta_df], axis=1)\n",
        "test = pd.concat([test, test_theta_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUNW2Y9iwnzw"
      },
      "source": [
        "train.excerpt = '[' + train.topic_id.map(str) + '] </s> '+train.excerpt\n",
        "test.excerpt =  '[' + test.topic_id.map(str) + '] </s> '+test.excerpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZrlSEKfdMtH"
      },
      "source": [
        "train.to_csv(\"train_lda.csv\",index=None)\n",
        "test.to_csv(\"test_lda.csv\",index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUZan8AdCInc"
      },
      "source": [
        "def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n",
        "    data = data.replace('\\n', '')\n",
        "    tok = tokenizer.encode_plus(\n",
        "        data, \n",
        "        max_length=max_len, \n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=True\n",
        "    )\n",
        "    curr_sent = {}\n",
        "    padding_length = max_len - len(tok['input_ids'])\n",
        "    curr_sent['input_ids'] = tok['input_ids'] + ([1] * padding_length)\n",
        "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
        "        ([0] * padding_length)\n",
        "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
        "        ([0] * padding_length)\n",
        "    return curr_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niK6D18uCSdn"
      },
      "source": [
        "class DatasetRetriever(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
        "        self.data = data\n",
        "        if 'excerpt' in self.data.columns:\n",
        "            self.excerpts = self.data.excerpt.values.tolist()\n",
        "        else:\n",
        "            self.excerpts = self.data.text.values.tolist()\n",
        "        self.targets = self.data.target.values.tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_test = is_test\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        excerpt, label = self.excerpts[item], self.targets[item]\n",
        "        features = convert_examples_to_features(\n",
        "            excerpt, self.tokenizer, \n",
        "            self.max_len, self.is_test\n",
        "        )\n",
        "        return {\n",
        "            'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
        "            'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
        "            'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
        "            'label':torch.tensor(label, dtype=torch.double),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH2K91nfCWLB"
      },
      "source": [
        "class CommonLitModel(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        model_name, \n",
        "        config,  \n",
        "        multisample_dropout=False,\n",
        "        output_hidden_states=False\n",
        "    ):\n",
        "        super(CommonLitModel, self).__init__()\n",
        "        self.config = config\n",
        "        self.roberta = AutoModel.from_pretrained(\n",
        "            model_name, \n",
        "            output_hidden_states=output_hidden_states\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
        "        if multisample_dropout:\n",
        "            self.dropouts = nn.ModuleList([\n",
        "                nn.Dropout(0.5) for _ in range(5)\n",
        "            ])\n",
        "        else:\n",
        "            self.dropouts = nn.ModuleList([nn.Dropout(0.1)])\n",
        "        #self.regressor = nn.Linear(config.hidden_size*2, 1)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
        "        self._init_weights(self.layer_norm)\n",
        "        self._init_weights(self.regressor)\n",
        " \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        " \n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        labels=None\n",
        "    ):\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "        sequence_output = outputs[1]\n",
        "        sequence_output = self.layer_norm(sequence_output)\n",
        " \n",
        "        # max-avg head\n",
        "        # average_pool = torch.mean(sequence_output, 1)\n",
        "        # max_pool, _ = torch.max(sequence_output, 1)\n",
        "        # concat_sequence_output = torch.cat((average_pool, max_pool), 1)\n",
        " \n",
        "        # multi-sample dropout\n",
        "        for i, dropout in enumerate(self.dropouts):\n",
        "            if i == 0:\n",
        "                logits = self.regressor(dropout(sequence_output))\n",
        "            else:\n",
        "                logits += self.regressor(dropout(sequence_output))\n",
        "        \n",
        "        logits /= len(self.dropouts)\n",
        " \n",
        "        # calculate loss\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # regression task\n",
        "            loss_fn = torch.nn.MSELoss()\n",
        "            logits = logits.view(-1).to(labels.dtype)\n",
        "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
        "        \n",
        "        output = (logits,) + outputs[1:]\n",
        "        return ((loss,) + output) if loss is not None else output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta2DtpTNChIG"
      },
      "source": [
        "\n",
        "class Lamb(Optimizer):\n",
        "    # Reference code: https://github.com/cybertronai/pytorch-lamb\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr: float = 1e-3,\n",
        "        betas = (0.9, 0.999),\n",
        "        eps: float = 1e-6,\n",
        "        weight_decay: float = 0,\n",
        "        clamp_value: float = 10,\n",
        "        adam: bool = False,\n",
        "        debias: bool = False,\n",
        "    ):\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
        "        if eps < 0.0:\n",
        "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\n",
        "                'Invalid beta parameter at index 0: {}'.format(betas[0])\n",
        "            )\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\n",
        "                'Invalid beta parameter at index 1: {}'.format(betas[1])\n",
        "            )\n",
        "        if weight_decay < 0:\n",
        "            raise ValueError(\n",
        "                'Invalid weight_decay value: {}'.format(weight_decay)\n",
        "            )\n",
        "        if clamp_value < 0.0:\n",
        "            raise ValueError('Invalid clamp value: {}'.format(clamp_value))\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.clamp_value = clamp_value\n",
        "        self.adam = adam\n",
        "        self.debias = debias\n",
        "\n",
        "        super(Lamb, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure = None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    msg = (\n",
        "                        'Lamb does not support sparse gradients, '\n",
        "                        'please consider SparseAdam instead'\n",
        "                    )\n",
        "                    raise RuntimeError(msg)\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # m_t\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                # v_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                # Paper v3 does not use debiasing.\n",
        "                if self.debias:\n",
        "                    bias_correction = math.sqrt(1 - beta2 ** state['step'])\n",
        "                    bias_correction /= 1 - beta1 ** state['step']\n",
        "                else:\n",
        "                    bias_correction = 1\n",
        "\n",
        "                # Apply bias to lr to avoid broadcast.\n",
        "                step_size = group['lr'] * bias_correction\n",
        "\n",
        "                weight_norm = torch.norm(p.data).clamp(0, self.clamp_value)\n",
        "\n",
        "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n",
        "                if group['weight_decay'] != 0:\n",
        "                    adam_step.add_(p.data, alpha=group['weight_decay'])\n",
        "\n",
        "                adam_norm = torch.norm(adam_step)\n",
        "                if weight_norm == 0 or adam_norm == 0:\n",
        "                    trust_ratio = 1\n",
        "                else:\n",
        "                    trust_ratio = weight_norm / adam_norm\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = adam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "                if self.adam:\n",
        "                    trust_ratio = 1\n",
        "\n",
        "                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eravelFCqWX"
      },
      "source": [
        "def get_optimizer_params(model):\n",
        "    # 微分学習率とウェイトディケイ\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    learning_rate = 5e-5\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
        "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
        "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.01, 'lr': learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.01, 'lr': learning_rate},\n",
        "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.01, 'lr': learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay_rate': 0.0},\n",
        "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.0, 'lr': learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.0, 'lr': learning_rate},\n",
        "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.0, 'lr': learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.named_parameters() if \"roberta\" not in n], 'lr':1e-3, \"momentum\" : 0.99},\n",
        "    ]\n",
        "    return optimizer_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzSCMdloCtg2"
      },
      "source": [
        "\n",
        "def make_model(model_name='../input/robertaitpt/', num_labels=1):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    config.update({'num_labels':num_labels})\n",
        "    model = CommonLitModel(model_name, config=config)\n",
        "    return model, tokenizer\n",
        "\n",
        "def make_optimizer(model, optimizer_name=\"AdamW\"):\n",
        "    optimizer_grouped_parameters = get_optimizer_params(model)\n",
        "    kwargs = {\n",
        "            'lr':3e-5,\n",
        "            'weight_decay':0\n",
        "    }\n",
        "    if optimizer_name == \"LAMB\":\n",
        "        optimizer = Lamb(optimizer_grouped_parameters, **kwargs)\n",
        "        return optimizer\n",
        "    elif optimizer_name == \"Adam\":\n",
        "        from torch.optim import Adam\n",
        "        optimizer = Adam(optimizer_grouped_parameters, **kwargs)\n",
        "        return optimizer\n",
        "    elif optimizer_name == \"AdamW\":\n",
        "        from torch.optim import AdamW\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, **kwargs)\n",
        "        return optimizer\n",
        "    else:\n",
        "        raise Exception('Unknown optimizer: {}'.format(optimizer_name))\n",
        "\n",
        "def make_scheduler(optimizer, decay_name='cosine_warmup', t_max=10):\n",
        "    if decay_name == 'step':\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer,\n",
        "            milestones=[30, 60, 90],\n",
        "            gamma=0.1\n",
        "        )\n",
        "    elif decay_name == 'cosine':\n",
        "        scheduler = lrs.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=t_max\n",
        "        )\n",
        "    elif decay_name == \"cosine_warmup\":\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=t_max\n",
        "        )\n",
        "    else:\n",
        "        raise Exception('Unknown lr scheduler: {}'.format(decay_type))    \n",
        "    return scheduler    \n",
        "\n",
        "def make_loader(\n",
        "    data, \n",
        "    tokenizer, \n",
        "    max_len,\n",
        "    batch_size,\n",
        "    fold=0\n",
        "):\n",
        "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
        "    train_dataset = DatasetRetriever(train_set, tokenizer, max_len)\n",
        "    valid_dataset = DatasetRetriever(valid_set, tokenizer, max_len)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        sampler=train_sampler, \n",
        "        pin_memory=True, \n",
        "        drop_last=False, \n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    valid_sampler = SequentialSampler(valid_dataset)\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset, \n",
        "        batch_size=batch_size // 2, \n",
        "        sampler=valid_sampler, \n",
        "        pin_memory=True, \n",
        "        drop_last=False, \n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    return train_loader, valid_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gj_e7waCxvH"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.max = 0\n",
        "        self.min = 1e5\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        if val > self.max:\n",
        "            self.max = val\n",
        "        if val < self.min:\n",
        "            self.min = val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCd8UAExC0SK"
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, scheduler, scalar=None, log_interval=1, evaluate_interval=1):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.scalar = scalar\n",
        "        self.log_interval = log_interval\n",
        "        self.evaluate_interval = evaluate_interval\n",
        "        self.evaluator = Evaluator(self.model, self.scalar)\n",
        "\n",
        "    def train(self, train_loader, valid_loader, epoch, \n",
        "              result_dict, tokenizer, fold):\n",
        "        total_loss = 0\n",
        "        count = 0\n",
        "\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        self.model.train()\n",
        "        \n",
        "        for batch_idx, batch_data in enumerate(train_loader):\n",
        "            input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n",
        "                batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n",
        "            input_ids, attention_mask, token_type_ids, labels = \\\n",
        "                input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
        "\n",
        "            \n",
        "            if self.scalar is not None:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        token_type_ids=token_type_ids,\n",
        "                        labels=labels\n",
        "                    )\n",
        "            else:\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    token_type_ids=token_type_ids,\n",
        "                    labels=labels\n",
        "                )\n",
        "            \n",
        "#             torch.nn.utils.clip_grad_norm_(\n",
        "#                 parameters=self.model.parameters(), \n",
        "#                 max_norm=1.0\n",
        "#             )\n",
        "\n",
        "            loss, logits = outputs[:2]\n",
        "            total_loss += loss.tolist()\n",
        "            count += labels.size(0)\n",
        "\n",
        "            logits = logits.cpu().detach().numpy()\n",
        "            losses.update(loss.item(), input_ids.size(0))\n",
        "            \n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            if self.scalar is not None:\n",
        "                self.scalar.scale(loss).backward()\n",
        "                self.scalar.step(self.optimizer)\n",
        "                self.scalar.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            if batch_idx % self.log_interval == 0:\n",
        "                _s = str(len(str(len(train_loader.sampler))))\n",
        "                ret = [\n",
        "                    ('epoch: {:0>3} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_loader.sampler), 100 * count / len(train_loader.sampler)),\n",
        "                    'train_loss: {: >4.5f}'.format(total_loss / count),\n",
        "                ]\n",
        "                print(', '.join(ret))\n",
        "            \n",
        "            if batch_idx % self.evaluate_interval == 0:\n",
        "                result_dict = self.evaluator.evaluate(\n",
        "                    valid_loader, \n",
        "                    epoch, \n",
        "                    result_dict, \n",
        "                    tokenizer\n",
        "                )\n",
        "                if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
        "                    print(\"{} epoch, best epoch was updated! valid_loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
        "                    result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]\n",
        "                    torch.save(self.model.state_dict(), f\"model{fold}.bin\")\n",
        "\n",
        "        self.scheduler.step()\n",
        "        result_dict['train_loss'].append(losses.avg)\n",
        "        return result_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzNyJYguC4o_"
      },
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model, scalar=None):\n",
        "        self.model = model\n",
        "        self.scalar = scalar\n",
        "    \n",
        "    def worst_result(self):\n",
        "        ret = {\n",
        "            'loss':float('inf'),\n",
        "            'accuracy':0.0\n",
        "        }\n",
        "        return ret\n",
        "\n",
        "    def result_to_str(self, result):\n",
        "        ret = [\n",
        "            'epoch: {epoch:0>3}',\n",
        "            'loss: {loss: >4.2e}'\n",
        "        ]\n",
        "        for metric in self.evaluation_metrics:\n",
        "            ret.append('{}: {}'.format(metric.name, metric.fmtstr))\n",
        "        return ', '.join(ret).format(**result)\n",
        "\n",
        "    def save(self, result):\n",
        "        with open('result_dict.json', 'w') as f:\n",
        "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
        "    \n",
        "    def load(self):\n",
        "        result = self.worst_result\n",
        "        if os.path.exists('result_dict.json'):\n",
        "            with open('result_dict.json', 'r') as f:\n",
        "                try:\n",
        "                    result = json.loads(f.read())\n",
        "                except:\n",
        "                    pass\n",
        "        return result\n",
        "\n",
        "    def evaluate(self, data_loader, epoch, result_dict, tokenizer):\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch_data in enumerate(data_loader):\n",
        "                input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n",
        "                    batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n",
        "                input_ids, attention_mask, token_type_ids, labels = input_ids.cuda(), \\\n",
        "                    attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
        "                \n",
        "                if self.scalar is not None:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        outputs = self.model(\n",
        "                            input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids,\n",
        "                            labels=labels\n",
        "                        )\n",
        "                else:\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        token_type_ids=token_type_ids,\n",
        "                        labels=labels\n",
        "                    )\n",
        "                \n",
        "                loss, logits = outputs[:2]\n",
        "                losses.update(loss.item(), input_ids.size(0))\n",
        "\n",
        "        print('----Validation Results Summary----')\n",
        "        print('Epoch: [{}] train_loss: {: >4.5f}'.format(epoch, losses.avg))\n",
        "\n",
        "        result_dict['val_loss'].append(losses.avg)        \n",
        "        return result_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQzWqXovC8ap"
      },
      "source": [
        "def config(fold=0):\n",
        "    torch.manual_seed(2021)\n",
        "    torch.cuda.manual_seed(2021)\n",
        "    torch.cuda.manual_seed_all(2021)\n",
        "    epochs = 8\n",
        "    max_len = 250\n",
        "    batch_size = 24\n",
        "\n",
        "    model, tokenizer = make_model(model_name='/content/drive/MyDrive/kaggle/readability/roberta-e1', num_labels=1)\n",
        "    optimizer = make_optimizer(model, \"AdamW\")\n",
        "    scheduler = make_scheduler(optimizer, t_max=epochs)\n",
        "    train_loader, valid_loader = make_loader(\n",
        "        train, tokenizer, max_len=max_len,\n",
        "        batch_size=batch_size, fold=fold\n",
        "    )\n",
        "\n",
        "    if torch.cuda.device_count() >= 1:\n",
        "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
        "            torch.cuda.device_count(), \n",
        "            torch.cuda.get_device_name(0))\n",
        "        )\n",
        "        model = model.cuda() \n",
        "    else:\n",
        "        raise ValueError('CPU training is not supported')\n",
        "\n",
        "    # scaler = torch.cuda.amp.GradScaler()\n",
        "    scaler = None\n",
        "\n",
        "    result_dict = {\n",
        "        'epoch':[], \n",
        "        'train_loss': [], \n",
        "        'val_loss' : [], \n",
        "        'test_loss': [],\n",
        "        'best_val_loss': np.inf\n",
        "    }\n",
        "    return (\n",
        "        model, tokenizer, \n",
        "        optimizer, scheduler, \n",
        "        scaler, train_loader, \n",
        "        valid_loader, result_dict, \n",
        "        epochs\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrUWqMfdC_BH"
      },
      "source": [
        "def run(fold=0):\n",
        "    model, tokenizer, optimizer, scheduler, scaler, \\\n",
        "        train_loader, valid_loader, result_dict, epochs = config(fold)\n",
        "    import time\n",
        "    trainer = Trainer(model, optimizer, scheduler, scaler)\n",
        "    train_time_list = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        result_dict['epoch'] = epoch\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        tic1 = time.time()\n",
        "\n",
        "        result_dict = trainer.train(train_loader, valid_loader, epoch, \n",
        "                                    result_dict, tokenizer, fold)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        tic2 = time.time() \n",
        "        train_time_list.append(tic2 - tic1)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    del model, tokenizer, optimizer, scheduler, \\\n",
        "        scaler, train_loader, valid_loader,\n",
        "    gc.collect()\n",
        "    return result_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-m-H5Bccp6p"
      },
      "source": [
        "result[\"best_val_loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MD9mL7KDAju"
      },
      "source": [
        "best_scores=[]\n",
        "for fold in range(5):\n",
        "    print('----')\n",
        "    print(f'FOLD: {fold}')\n",
        "    result=run(fold)\n",
        "    print('----')\n",
        "    print()\n",
        "    best_scores.append(result[\"best_val_loss\"])\n",
        "print(f\"CV:{np.mean(best_scores)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p2W4sjDIPh9"
      },
      "source": [
        "!mkdir /content/drive/MyDrive/kaggle/readability/robertalda\n",
        "!cp /content/model0.bin /content/drive/MyDrive/kaggle/readability/robertalda\n",
        "!cp /content/model1.bin /content/drive/MyDrive/kaggle/readability/robertalda\n",
        "!cp /content/model2.bin /content/drive/MyDrive/kaggle/readability/robertalda\n",
        "!cp /content/model2.bin /content/drive/MyDrive/kaggle/readability/robertalda\n",
        "!cp /content/model3.bin /content/drive/MyDrive/kaggle/readability/robertalda\n",
        "!cp /content/model4.bin /content/drive/MyDrive/kaggle/readability/robertalda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Pg9r-h4RUv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}